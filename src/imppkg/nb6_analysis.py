# -*- coding: utf-8 -*-
"""sentiment_arcs_part6_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13elH_S2deF0e6-7DsQyRx-b7LPb_Ri5m

# **SentimentArcs (Part 6): Analysis**

By: Jon Chun
* Original: 12 Jun 2021
* Last Update: 20 Apr 2022

# [STEP 1] Manual Configuration

## (Popups) Connect Google gDrive
"""

# [INPUT REQUIRED]: Authorize access to Google gDrive

# Connect this Notebook to your permanent Google Drive
#   so all generated output is saved to permanent storage there

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("Attempting to attach your Google gDrive to this Colab Jupyter Notebook")
  drive.mount('/gdrive')
else:
  print("Your Google gDrive is attached to this Colab Jupyter Notebook")

"""## (3 Inputs) Define Directory Tree"""

# Commented out IPython magic to ensure Python compatibility.
# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)
#              to math the full path to your gDrive subdirectory which should be the 
#              root directory cloned from the SentimentArcs github repo.

# NOTE: Make sure this subdirectory already exists and there are 
#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd

# NOTE: In Python all strings must begin with an upper or lowercase letter, and only
#         letter, number and underscores ('_') characters should appear afterwards.
#         Make sure your full path after %cd obeys this constraint or errors may appear.

# #@markdown **Instructions**

# #@markdown Set Directory and Corpus names:
# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**
# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]
# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]
# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)

#@markdown <hr>

# Step #1: Get full path to SentimentArcs subdir on gDrive
# =======
#@markdown **Accept default path on gDrive or Enter new one:**

Path_to_SentimentArcs = "/gdrive/MyDrive/sentimentarcs_notebooks/" #@param ["/gdrive/MyDrive/sentiment_arcs/"] {allow-input: true}


#@markdown Set this to the project root in your <b>GDrive folder</b>
#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)

#@markdown <hr>

#@markdown **Which type of texts are you cleaning?** \

Corpus_Genre = "novels" #@param ["novels", "social_media", "finance"]

# Corpus_Type = "reference" #@param ["new", "reference"]
Corpus_Type = "new" #@param ["new", "reference"]


Corpus_Number = 3 #@param {type:"slider", min:1, max:10, step:1}


#@markdown Put in the corresponding Subdirectory under **./text_raw**:
#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files 
#@markdown <li> A <b>YAML Configuration File</b> describing each Texts

#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.

print('Current Working Directory:')
# %cd $Path_to_SentimentArcs

print('\n')

if Corpus_Type == 'reference':
  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'
  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'
else:
  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'
  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'

# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'
# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'
PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'
PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'

# TODO: Clean up
# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN

print(f'PATH_SENTIMENT_RAW:\n  [{PATH_SENTIMENT_RAW}]')
print(f'SUBDIR_SENTIMENT_RAW:\n  [{SUBDIR_SENTIMENT_RAW}]')

print('\n')

print(f'PATH_TEXT_CLEAN:\n  [{PATH_TEXT_CLEAN}]')
print(f'SUBDIR_TEXT_CLEAN:\n  [{SUBDIR_TEXT_CLEAN}]')

"""# **[STEP 2] Automatic Configuration/Setup**

## (each time) Custom Libraries & Define Globals
"""

# Add PATH for ./utils subdirectory

import sys
import os

!python --version

print('\n')

PATH_UTILS = f'{Path_to_SentimentArcs}utils'
PATH_UTILS

sys.path.append(PATH_UTILS)

print('Contents of Subdirectory [./sentiment_arcs/utils/]\n')
!ls $PATH_UTILS

# More Specific than PATH for searching libraries
# !echo $PYTHONPATH

# Review Global Variables and set the first few

import global_vars as global_vars

global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs
global_vars.Corpus_Genre = Corpus_Genre
global_vars.Corpus_Type = Corpus_Type
global_vars.Corpus_Number = Corpus_Number

global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW
global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW

global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN
global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN

from utils import sa_config # (e.g. define TEST_WORDS_LS)

sa_config.set_globals()

global_vars.TEST_WORDS_LS
print('\n')

dir(global_vars)

# Commented out IPython magic to ensure Python compatibility.
# %whos dict

# Initialize and clean for each iteration of notebook

# dir(global_vars)

global_vars.corpus_texts_dt = {}
global_vars.corpus_titles_dt = {}

# Import SentimentArcs Utilities to define Directory Structure
#   based the Selected Corpus Genre, Type and Number

!pwd 
print('\n')

# from utils import sa_config # .sentiment_arcs_utils
from utils import sa_config

print('Objects in sa_config()')
print(dir(sa_config))
print('\n')

# Directory Structure for the Selected Corpus Genre, Type and Number
sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')

global_vars.SUBDIR_SENTIMENT_CLEAN

# TODO: correct typo in config file

global_vars.SUBDIR_SENTIMENT_CLEAN = './sentiment_clean/sentiemnt_clean_novels_new_corpus2/'
global_vars.SUBDIR_SENTIMENT_CLEAN

"""## (each time) Read YAML Configuration for Corpus and Models """

# from utils import sa_config # .sentiment_arcs_utils

import yaml

from utils import read_yaml

print('Objects in read_yaml()')
print(dir(read_yaml))
print('\n')

# Directory Structure for the Selected Corpus Genre, Type and Number
read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)

print('SentimentArcs Model Ensemble ------------------------------\n')
model_titles_ls = global_vars.models_titles_dt.keys()
print('\n'.join(model_titles_ls))


print('\n\nCorpus Texts ------------------------------\n')
corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())
print('\n'.join(corpus_titles_ls))


print(f'\n\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\n')
print(f'\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\n')
print('\n')

global_vars.corpus_titles_dt

global_vars.models_titles_dt.items()

global_vars.corpus_titles_dt

"""## Configure Jupyter Notebook"""

# Commented out IPython magic to ensure Python compatibility.
# Configure Jupyter

# To reload modules under development

# Option (a)
# %load_ext autoreload
# %autoreload 2
# Option (b)
# import importlib
# importlib.reload(functions.readfunctions)


# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Enable multiple outputs from one code cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

from IPython.display import display
from IPython.display import Image
from ipywidgets import widgets, interactive

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Intentionally left blank

"""## Load Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np

from tqdm._tqdm_notebook import tqdm_notebook
import pandas as pd
tqdm_notebook.pandas()

import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline
pd.set_option('max_colwidth', 100) # -1)

import string
string.punctuation

from glob import glob
import json
from collections import Counter
import math

# import copy

from IPython.display import Markdown, display

from ipywidgets import interact, Dropdown, Select
from ipywidgets import Layout, interact, IntSlider

import plotly.graph_objects as go  # for data visualization
import plotly.express as px  # for data visualization 
from plotly.subplots import make_subplots

from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

scaler_zscore = StandardScaler()
scaler_minmax = MinMaxScaler()
scaler_robust = RobustScaler()  # Unused currently

import statsmodels.robust.scale as sm_robust
# import statsmodels.api as sm # to build a LOWESS model
from statsmodels.nonparametric.smoothers_lowess import lowess

from scipy.signal import find_peaks
from scipy import signal

# from scipy.interpolate import interp1d # for interpolation of new data points

"""## Setup Matplotlib Style

* https://matplotlib.org/stable/tutorials/introductory/customizing.html
"""

# Commented out IPython magic to ensure Python compatibility.
# Configure Matplotlib

# View available styles
# plt.style.available

# Verify in SentimentArcs Root Directory
os.chdir(Path_to_SentimentArcs)

# %run -i './utils/config_matplotlib.py'

config_matplotlib()

print('Matplotlib Configuration ------------------------------')
print('\n  (Uncomment to view)')
# plt.rcParams.keys()
print('\n  Edit ./utils/config_matplotlib.py to change')

"""## Setup Seaborn Style"""

# Commented out IPython magic to ensure Python compatibility.
# Configure Seaborn

# Verify in SentimentArcs Root Directory
os.chdir(Path_to_SentimentArcs)

# %run -i './utils/config_seaborn.py'

config_seaborn()

print('Seaborn Configuration ------------------------------\n')
# print('\n  Update ./utils/config_seaborn.py to display seaborn settings')



"""## Python Utility Functions

### (each time) Generate Convenient Data Lists
"""

# Derive List of Texts in Corpus a)keys and b)full author and titles

print('Dictionary: corpus_titles_dt')
global_vars.corpus_titles_dt
print('\n')

corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())
print(f'\nCorpus Texts:')
for akey in corpus_texts_ls:
  print(f'  {akey}')
print('\n')

print(f'\nNatural Corpus Titles:')
corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]
for akey in corpus_titles_ls:
  print(f'  {akey}')

global_vars.corpus_titles_dt.keys()

# Get Model Families of Ensemble

from utils.get_model_families import get_ensemble_model_famalies

global_vars.model_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)

print('\nTest: Lexicon Family of Models:')
global_vars.model_ensemble_dt['lexicon']

"""### File Functions"""

# Commented out IPython magic to ensure Python compatibility.
# Verify in SentimentArcs Root Directory
os.chdir(Path_to_SentimentArcs)

# %run -i './utils/file_utils.py'
# from utils.file_utils import *

# %run -i './utils/file_utils.py'

# TODO: Not used? Delete?
# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)

"""# **[STEP 3] Read all Raw Sentiment Data**

## Read Raw Sentiments
"""

# Verify cwd and subdir of Raw Sentiment Data

print('Current Working Directory:')
!pwd

print(f'\nSubdir with all Cleaned Texts of Corpus:\n  {SUBDIR_SENTIMENT_RAW}')

PATH_SENTIMENT_RAW = f'{Path_to_SentimentArcs}sentiment_raw/{SUBDIR_SENTIMENT_RAW}'

print(f'\nPATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\n')

print(f'\n\nFilenames of Cleaned Texts:\n')
!ls -1 $PATH_SENTIMENT_RAW

# glob(f'{PATH_SENTIMENT_RAW}/*')

print('\n')

print(corpus_texts_ls)

# Create a List (sentiment_raw_json_ls) of all preprocessed text files

# Verify in SentimentArcs Root Directory
os.chdir(Path_to_SentimentArcs)

try:
    sentiment_raw_json_ls = glob(f'{PATH_SENTIMENT_RAW}/sentiment_raw_*.json')
    sentiment_raw_json_ls = [x.split('/')[-1] for x in sentiment_raw_json_ls]
    # sentiment_raw_json_ls = [x.split('.')[0] for x in sentiment_raw_json_ls]
except IndexError:
    raise RuntimeError('No csv file found')

print('\n'.join(sentiment_raw_json_ls))
print('\n')
print(f'Found {len(sentiment_raw_json_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')

# Global Dict for Sentiments

# Only used in this Notebook so not in defined in shared utils/global_vars
#   like global_vars.corpus_texts_dt = {}

# corpus_sentiments_dt[text] = DataFrame(Raw Sentiments, 1 Column per Model)

corpus_sentiment_dt = {}

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # NOTE:   2m37s @09:32 on 20220416 Colab Pro CPU (634k, 668k, 909k)
# #         2m07s @10:07 on 20220416 Colab Pro CPU (634k, 668k, 909k)
# #         2m07s @10:09 on 20220416 Colab Pro CPU (634k, 668k, 909k)
# #         2m03s @10:18 on 20220417 Colab Pro CPU (634k, 668k, 909k)
# 
# 
# # Read all preprocessed text files into master DataFrame (corpus_dt)
# 
# # Reset Dict for Sentiments
# #   Only used in this notebook, not shared across notebooks so do not
# #   share via utils/global_vars like global_vars.corpus_texts_dt
# 
# corpus_sentiment_dt = {}
# 
# for i, atext in enumerate(corpus_texts_ls):
#   print(f'\n\nProcessing text #{i}: {atext}')
#   corpus_sentiment_dt[atext] = pd.DataFrame(columns=['text_raw','text_clean'])
# 
#   for j, ajson in enumerate(sentiment_raw_json_ls):
#     print(f'  Reading json #{j}: {ajson}')
# 
#     afile_fullpath = f'{PATH_SENTIMENT_RAW}{ajson}'
#     print(f'               at: {afile_fullpath}')
# 
#     if 'transformer' in ajson:
#       print(f'   One Model Transformer *.json datafile')
#     else:
#       print(f'   Multi-Model non-Transformer *.json datafile')
# 
#     with open(afile_fullpath) as fp:
#       json_dt = json.load(fp)
#       temp_df = pd.DataFrame.from_dict(json_dt[atext]).reset_index()
#       # temp_df.head(5)
#       # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)
#       
#       # corpus_sentiment_dt[atext]
#       # print(f'               type: {json_dt[atext]}')
# 
#     # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)
#     corpus_sentiment_dt[atext] = pd.concat([corpus_sentiment_dt[atext], temp_df], axis=1).T.drop_duplicates().T #  = corpus_sentiment_dt[atext].update(temp_df)
#     # pd.concat([DF1, DF2], axis = 1).T.drop_duplicates().T
#     # corpus_sentiment_dt[atext] = pd.DataFrame.from_dict(json_dt)
# 
#   # ajson_df = pd.read_csv(afile_fullpath, index_col=[0])
#   # global_vars.corpus_texts_dt[atext] = ajson_df
#   # corpus_sentiment_dt[atext] = ajson_df
# 
# 
#   # a_json = json.loads(json_string)
#   # print(a_json)
# 
#

# Summary

title_indx = 0

corpus_sentiment_dt[corpus_texts_ls[title_indx]].info()

corpus_sentiment_dt[corpus_texts_ls[title_indx]].head()

print(f'For Text: {corpus_texts_ls[title_indx]}')

"""## Identify and Drop Duplicate Columns"""

# Drop all but the i-th copy of duplicated column

def keep_nthdup_col(adf, acol, nthcopy):
  '''
  Given a DataFrame, duplicated col name and nthcopy into set of duplicated cols
  Drop the iloc version of the duplicated col list from the DataFrame
  '''

  df_col_iloc_ls = []

  # First, verify this is a duplicated column
  col_dup_ls = [x for x in corpus_sentiment_dt[atext].columns if acol == x]
  if len(col_dup_ls) <= 1:
    print(f'ERROR: Column: {acol} is not duplicated in the DataFrame cols: {adf.columns}')
    return

  # Loop over all columns to get original iloc of duplicated columns
  # corpus_sentiment_dt[atext].columns.get_loc('roberta15lg')  # Return List of booleans

  for i in range(adf.shape[1]):

    # get current col name
    acol_name = adf.columns[i]

    # if current col name matches our target col, save it
    if acol_name == acol:
      # save the iloc
      df_col_iloc_ls.append(i)

  # Second, verify iloc points to one of the duplicated columns
  if nthcopy >= len(df_col_iloc_ls):
    print(f'ERROR: passed nthcopy {nthcopy} is bigger than the number of duplicated {acol} column [0 to {len(df_col_iloc_ls)-1}]')
    return

  print(f' Duplicated col: {acol} indicies: {df_col_iloc_ls}')
  col_dup_indx = df_col_iloc_ls[nthcopy]
  print(f'     Keep Index: {col_dup_indx}')
  print(f'           Name: {adf.columns[col_dup_indx]}')
  df_col_iloc_ls.remove(col_dup_indx)
  print(f'      Drop Cols: {df_col_iloc_ls}')
  # Drop all cols by iloc index in list df_col_iloc_ls
  # adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j not in df_col_iloc_ls]]
  for acol_indx in df_col_iloc_ls:
    adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j != int(acol_indx)]]

  """
  for k, acol_indx in enumerate(df_col_iloc_ls):
    acol_drop = adf.columns[acol_indx]
    print(f'Dropping column #{k}: {acol_drop} at indx={acol_indx}')
    # adf.drop(adf.columns[acol_indx], axis=1, inplace=True)
    adf.drop(columns=[acol_drop], axis=1, inplace=True)
  """

  return adf

# Test
# keep_nthdup_col(corpus_sentiment_dt[atext], 'text_raw', 1)

# Summary

corpus_sentiment_dt[atext].iloc[:, [j for j,c in enumerate(list(corpus_sentiment_dt[atext].columns)) if j not in [13,0]]].info()

# Identify and Drop Duplicate Columns

col_before_ct = len(corpus_sentiment_dt[atext].columns)
dup_col_keep_dt = {}  # Dict[dup_col] = iloc index to keep (col with min nulls)


for i,atext in enumerate(corpus_texts_ls):
  cols_dup_ls = []
  row_ct = corpus_sentiment_dt[atext].shape[0]

  print(f'\n\nProcessing Text #{i}: {atext}')
  
  # Count the frequency of each column name
  cols_ls = corpus_sentiment_dt[atext].columns
  # print(f'  Columns: {cols_ls}')
  col_count_dt = Counter(cols_ls)

  # Create list of duplicate column names in cols_dup_ls
  for key,val in col_count_dt.items():
    if val > 1:
      cols_dup_ls.append(key)
      print(f'  Duplicate col: {key} with count: {val}')

  # Count how many columns are duplicated
  dup_ct = len(cols_dup_ls)

  # For every duplicated Column
  for j, adup_col in enumerate(cols_dup_ls):
    # Count how many duplicates it has
    adup_col_ct = len(corpus_sentiment_dt[atext][adup_col])

    # Iterate through all duplicates and find the iloc index of the one
    #   with the least number of null values as the one to keep (deleting the other dups)
    col_iloc_min_null = 0  # Index to the col with min nulls
    col_min_null_ct = row_ct  # Current count of null in col with min nulls, init to row count
    dup_col_ls = corpus_sentiment_dt[atext][adup_col].columns
    for k, adup_col_ver in enumerate(dup_col_ls):
      adup_col_null_ct = corpus_sentiment_dt[atext][adup_col].iloc[:,k].isna().sum()
      if adup_col_null_ct < col_min_null_ct:
        col_min_null_ct = adup_col_null_ct
        col_iloc_min_null = k

    # Drop all but one copy of the duplicated columns
    print(f'\n      Keep iloc: {col_iloc_min_null} in adup_col: {adup_col} with {adup_col_null_ct} nulls out of {row_ct}')
    dup_col_keep_dt[adup_col] = col_iloc_min_null
    print(f'       Calling: keep_nthdup_col(adf, {adup_col}, {col_iloc_min_null})')
    corpus_sentiment_dt[atext] = keep_nthdup_col(corpus_sentiment_dt[atext], adup_col, col_iloc_min_null)


col_after_ct = len(corpus_sentiment_dt[atext].columns)

print(f'\n\nColumn Count:\n  Before: {col_before_ct}\n   After: {col_after_ct}')

"""## Reorder and Specify dtypes"""

# Get list of models

models_ls = list(set(corpus_sentiment_dt[corpus_texts_ls[0]].columns) - set(['text_raw','text_clean','index','sentence_no']))
models_ls.sort()

models_ls

print(f'\n\nTotal of {len(models_ls)} Models')

# Put text_raw and text_clean at front

# corpus_sentiment_dt[atext].sort_index(axis=1)
# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(0, 'text_raw', corpus_sentiment_dt[atext].pop('text_raw'))
# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(1, 'text_clean', corpus_sentiment_dt[atext].pop('text_clean'))

for i,atext in enumerate(corpus_texts_ls):

  col_first = corpus_sentiment_dt[atext].pop('index')
  corpus_sentiment_dt[atext].insert(0, 'sentence_no', col_first)

  col_second = corpus_sentiment_dt[atext].pop('text_raw')
  corpus_sentiment_dt[atext].insert(1, 'text_raw', col_second)

  col_third = corpus_sentiment_dt[atext].pop('text_clean')
  corpus_sentiment_dt[atext].insert(2, 'text_clean', col_third)

  corpus_sentiment_dt[atext].info()

# Convert objects to more specific dtypes

for i,atext in enumerate(corpus_texts_ls):
  print(f'\n\nProcessing Text #{i}: {atext}')

  for j, amodel in enumerate(models_ls):
  
    print(f'Processing Model #{j}: {amodel}')

    corpus_sentiment_dt[atext][amodel] = corpus_sentiment_dt[atext][amodel].astype('float')

  corpus_sentiment_dt[atext]['sentence_no'] = corpus_sentiment_dt[atext]['sentence_no'].astype('int')
  corpus_sentiment_dt[atext].info()

# Verify sample DataFrame

corpus_sentiment_dt[corpus_texts_ls[0]].head()

"""## Verify Raw Plots"""

# Verify Raw Sentiments with 

win_per = 10

for i,atext in enumerate(corpus_texts_ls):
  
  win_aper = int(win_per/100 * corpus_sentiment_dt[atext].shape[0])
  _ = corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()
  _= plt.title(f'Sentiment Analysis\n{global_vars.corpus_titles_dt[atext][0]}\nSmoothed SMA ({win_per}%)')
  plt.grid(True)

print(f'Read Raw Sentiments for these texts:\n  {corpus_sentiment_dt.keys()}\n\n')

"""## Drop or Interpolate and NaN/None Values"""

# Verify

corpus_sentiment_dt[corpus_texts_ls[0]]

# Drop Columns/Models with %NaN above Threshold

null_threshold = 0.9  # Drop Col if %rows=null > Threshold

for i,atext in enumerate(corpus_texts_ls):
  print(f'\n\nProcessing Text #{i}: {atext}')

  for j, amodel in enumerate(models_ls):
  
    # print(f'Processing Model #{j}: {amodel}')

    row_ct = len(corpus_sentiment_dt[atext][amodel])
    sum_null = corpus_sentiment_dt[atext][amodel].isnull().sum()
    # print(f'There are {sum_null} null values of a total {row_ct} rows')
    null_threshold = 0.5  # if > 50% null, drop col
    # print(f'Threshold: {null_threshold} of all {row_ct} rows')
    if sum_null > int(null_threshold * row_ct):
      print(f'  %NaNs above Threshold={null_threshold}: {corpus_sentiment_dt[atext][amodel].isna().sum()}')
      # TODO: Verify before dropping Col/Model here
      # corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()

"""## Clip Outliers and zScore Standardize"""

# Simple IQR

def clip_iqr_outliers(floats_ser, iqr_limit=1.5):
  '''
  Given a Pandas Series of floats and an upper limit on IQR variance from the median
  Clip all outliers beyond the iqr_limit and return a list of floats
  '''

  quantile10 = floats_ser.quantile(0.10)
  quantile90 = floats_ser.quantile(0.90)
  print(f'10% Quantile: {quantile10}')
  print(f'90% Quantile: {quantile90}')

  floats_np = np.where(floats_ser < quantile10, quantile10, floats_ser)
  floats_np = np.where(floats_ser > quantile90, quantile90, floats_ser)
  print(f'        Skew: {pd.Series(floats_np).skew()}')

  return floats_np # .tolist()

# Test

test_np = clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]][models_ls[0]])
len(test_np)

corpus_sentiment_dt[corpus_texts_ls[0]][models_ls[0]].quantile(0.10)

clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]][models_ls[0]],iqr_limit=1.5) # .values.reshape(-1, 1) )

corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']]

# Summary

corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns

# Trim Outliers, zScore Standardize and Create 'rz' versions

corpus_sentiment_rz_dt = {}

for i, atext in enumerate(corpus_texts_ls):
  # atext_rz_df = corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']].copy(deep=True)
  # col_rzscores_ls = []
  print(f"Title #{i}: {atext}")
  # df = corpus_sentiment_dt[atext].copy()
  # numeric_cols_ls = list(corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns) # .remove('sentence_no')
  # numeric_cols_ls.remove('sentence_no')

  # for anum_col_str in numeric_cols_ls:
  for j,amodel in enumerate(models_ls):
    print(f'  Model #{j}: {amodel}')
    # anum_col_robust_np = scaler_robust.fit_transform(df[amodel].values.reshape(-1, 1) )
    arobust_col_np = clip_iqr_outliers(corpus_sentiment_dt[atext][amodel],iqr_limit=1.5)
    # scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))
    # arobust_zscaled_col_np = scaler_zscore.fit_transform(arobust_col_np)
    arobust_zscaled_col_np = scaler_zscore.fit_transform(arobust_col_np.reshape(-1,1))
    arobust_zscaled_col_str = f'{amodel}_rz'
    corpus_sentiment_dt[atext][arobust_zscaled_col_str] = pd.Series(arobust_zscaled_col_np.squeeze(-1,))
  # corpus_sentiment_rz_dt[atext] = atext_rz_df

  # anum_col_rzscore_np = scaler_zscore.fit_transform(anum_col_robust_np)
  # anum_col_rzscore_str = f'{anum_col_str}_rzscore'
  # df[anum_col_rzscore_str] = pd.Series(anum_col_rzscore_np.squeeze(-1,))
  # col_rzscores_ls.append(anum_col_rzscore_str)

  # print(f'df.columns: {df.columns}')
  # win_10per = int(0.10 * df.shape[0])
  # df[col_rzscores_ls].rolling(win_10per, center=True, min_periods=0).mean() # .plot(title=f"Sentiment Analysis\n{global_vars.corpus_texts_dt[atext][0]}\nProcessing: SMA 10% (+ Robust IQR, zScore Scaling)")

models_rz_ls = [f'{x}_rz' for x in models_ls]
models_rz_ls

# Delete extraneous [_rz]*_rz cols if created from multiple runs

models_rz_ls = [x for x in corpus_sentiment_dt[corpus_texts_ls[0]] if 'rz' in x and 'rz_rz' not in x]
models_rz_ls = [x for x in models_rz_ls if 'sentence_no' not in x]
models_rz_ls

sma_percent = widgets.IntSlider(
    value=10,
    min=2,
    max=20,
    step=1,
    description='Percent:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='d'
)
print('Select Smoothing SMA Window Size:')
sma_percent

# Verify SMA Sentiment Arcs

text_indx = 0
text_str = corpus_texts_ls[text_indx]
title_str = global_vars.corpus_titles_dt[text_str][0]
win_per = sma_percent.value
win_size = int(win_per/100 * corpus_sentiment_dt[text_str].shape[0])

_ = corpus_sentiment_dt[text_str][models_rz_ls].rolling(win_size, center=True, min_periods=0).mean().plot(alpha=0.3)
_ = corpus_sentiment_dt[text_str][models_rz_ls].mean(axis=1).rolling(win_size, center=True, min_periods=0).mean().plot(label='mean', color='red', linewidth=3, alpha=0.7)
_ = plt.legend(loc='best')
_ = plt.title(f'Sentiment Arc: {title_str}\nSmoothed SMA ({win_per}%)')
plt.grid(True)

"""### **Save Checkpoint**"""

# TODO: Norm all paths and subdirs as 'dir/dir/dir/' except for root: '/dir/dir/dir/'

if Corpus_Type == 'new':
  global_vars.SUBDIR_SENTIMENT_CLEAN = f'sentiment_clean/sentiment_clean_{Corpus_Genre}_new_corpus{Corpus_Number}/'
elif Corpus_Type == 'reference':
  global_vars.SUBDIR_SENTIMENT_CLEAN = f'sentiment_clean/sentiment_clean_{Corpus_Genre}_reference/'
else:
  print(f'ERROR: Illegal value Corpus_Genre: {Corpus_Genre}')

print(f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}')

# Verify in SentimentArcs Root Directory
os.chdir(Path_to_SentimentArcs)

print('Currently in SentimentArcs root directory:')
!pwd

print(f'\nSaving Text_Type: {Corpus_Genre}')
print(f'     Corpus_Type: {Corpus_Type}')

# Verify Subdir to save Cleaned Texts and Texts into..

print(f'\nThese Text Titles:')
list(corpus_sentiment_dt.keys())

print(f'\n\nTo This Subdirectory:\n  {global_vars.SUBDIR_SENTIMENT_CLEAN}')

full_path = f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}'
print(f'\nFull path to this Subdirectory:\n  {full_path}')

if Corpus_Type == 'new':
  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all.json'
else:
  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_reference_all.json'
print(f'\nUnder this Filename:\n  {save_filename}')

write_dict_dfs(corpus_sentiment_dt, out_file=save_filename, out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}/')  # broken with novels_corpus4
# write_dict_dfs(corpus_sentiment_dt, out_file=f'{save_filename}', out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}')

# Verify json file created

!ls -altr $global_vars.SUBDIR_SENTIMENT_CLEAN

"""# **[STEP 4] Smoothing EDA**

## EDA: Multiple SMA Window Sizes
"""

selected_text = widgets.Dropdown(
    options=corpus_texts_ls,
    value=corpus_texts_ls[0],
    description='Text:',
    disabled=False,
)
selected_text

selected_model = widgets.Dropdown(
    options=models_ls,
    value=models_ls[0],
    description='Model:',
    disabled=False,
)
selected_model

# TODO: Add check to require min timeseries length to get meaningful smoothing
#   For now, use math.ceil to round up and avoid win_1per=0 errors


_ = plt.figure(figsize=(20,10))

win_1per = math.ceil(1/100 * corpus_sentiment_dt[selected_text.value].shape[0])
# win_1per = int(1/100 * corpus_sentiment_dt[selected_text.value].shape[0])

win_range_ls = [5, 10]

for i, awin_size in enumerate(win_range_ls):
  # win_size = math.ceil(awin_size * win_1per)
  win_size = int(awin_size * win_1per)

  title_str = global_vars.corpus_titles_dt[selected_text.value][0]
  
  _ = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean().plot(label=win_size)
  _ = plt.title(f'Sentiment Arc: {title_str}\nModel: {selected_model.value}\nSmoothing: SMA ({win_range_ls}%)')
  _ = plt.legend(loc='best', title='Window Size')
  plt.grid(True)

"""## EDA: Multiple Sentiment Arcs"""

# Select which Models to Compare

print(f'Select which Models to compare (at least one):')
models_ls.sort()
models_nosentno_ls = [x for x in models_ls if 'sentence_no' not in x]
models_selected_ls = [widgets.Checkbox(value=True, description=label) for label in models_nosentno_ls]
models_sel_vbox = widgets.VBox(children=models_selected_ls)
display(models_sel_vbox)


display(Markdown('---'))
print('Select which Text to Analyze:')
if (len(corpus_texts_ls) <= 1):
  print(f'  {corpus_texts_ls[0]} (there is only one Text in this Corpus)')
else:
  corpus_texts_ls.sort()
  selected_text = widgets.Dropdown(
      options=corpus_texts_ls,
      value=corpus_texts_ls[0],
      description='Model:',
      disabled=False,
  )

display(Markdown('---'))
print('Select a SMA Window Size:')
selected_sma_window = widgets.IntSlider(
    value=10,
    min=2, # max 
    max=20, # min 
    step=1, # step
    description='Percent'
)
selected_sma_window

models_subset_ls = []

for i in range(0, len(models_selected_ls)):
  if models_selected_ls[i].value == True:
    models_subset_ls.append(models_selected_ls[i].description + '_rz')

print(f'Selected Models:\n')

models_subset_ls

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # NOTE: 1m12s
# 
# # Plot Selected Normed Models for Chosen Text (using SMA Smoothing)
# 
# subensemble_df = pd.DataFrame(corpus_sentiment_dt[selected_text.value].rolling(win_size, center=True, min_periods=0).mean())
# subensemble_df['sentence_no'] = corpus_sentiment_dt[selected_text.value]['sentence_no']
# subensemble_df['text_raw'] = corpus_sentiment_dt[selected_text.value]['text_raw']
# 
# _ = fig = px.line(subensemble_df, x='sentence_no', y=models_subset_ls, custom_data=['text_raw']) #  labels={'x':'Sentence No', 'y':'Sentiment'})
# 
# # Update axes lines
# _ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
#                  zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
#                  showline=True, linewidth=1, linecolor='black')
# 
# _ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
#                  zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
#                  showline=True, linewidth=1, linecolor='black')
# 
# title_str = global_vars.corpus_titles_dt[selected_text.value][0]
# title_full_str = f'SentimentArcs: {title_str}<br>Ensemble of {len(models_subset_ls)} with {selected_sma_window.value}% SMA'
# 
# _ = fig.update_layout(
#     plot_bgcolor='white',
#     title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),
#     xaxis_title="Line No",
#     yaxis_title='Sentiment',
#     xaxis=dict(showgrid=True), 
#     yaxis=dict(showgrid=True),
#     legend=dict(
#     title = "Models:",
#     orientation="h",
#     xanchor="center",
#     yanchor="bottom",
#     y=-0.3, # 0.99,
#     x=0.5 # x=0.01
# ))
# 
# 
# _ = fig.update_traces(
#     hovertemplate="<br>".join([
#         "Sentence No: %{x}",
#         "Norm Sentiment: %{y}",
#         "Text: %{customdata[0]}",
#     ])
# )
# 
# # Show plot 
# fig.show()

"""## EDA: (Sub)Ensemble Coherence"""

# Summary

corpus_texts_ls = list(corpus_sentiment_dt.keys())
print(f'corpus_texts_ls:\n  {corpus_texts_ls}\n\n')

text_first = corpus_texts_ls[0]
print(f'text_first:\n  {text_first}\n\n')

cols_all_ls = list(corpus_sentiment_dt[text_first].columns)
models_rz_ls = [x for x in cols_all_ls if '_rz' in x]
print(f'models_rz_ls:\n  {models_rz_ls}\n\n')

# Calculate Coherence for Ensemble and Subensemble for Currently Selected Text

print(f'For Currently Selected Text:\n  {selected_text.value}\n')

print(f' Full Ensemble has:\n    {len(models_rz_ls)} Models')
print(f' SubEnsemble has:\n    {len(models_subset_ls)} Models\n')

print('Models Removed from SubEnsemble:')
models_dropped_ls = list(set(models_rz_ls) - set(models_subset_ls))
models_dropped_ls
print('\n')

coherence_df = pd.DataFrame()
coherence_df['sentence_no'] = corpus_sentiment_dt[selected_text.value]['sentence_no']
coherence_df['text_raw'] = corpus_sentiment_dt[selected_text.value]['text_raw']
coherence_df['subset_range'] = pd.Series(corpus_sentiment_dt[selected_text.value][models_subset_ls].max(axis=1) - corpus_sentiment_dt[selected_text.value][models_subset_ls].min(axis=1))
coherence_df['subset_mean'] = pd.Series(corpus_sentiment_dt[selected_text.value][models_subset_ls].mean(axis=1))
coherence_df['ensemble_range'] = pd.Series(corpus_sentiment_dt[selected_text.value][models_rz_ls].max(axis=1) - corpus_sentiment_dt[selected_text.value][models_rz_ls].min(axis=1))
coherence_df['ensemble_mean'] = pd.Series(corpus_sentiment_dt[selected_text.value][models_rz_ls].mean(axis=1))

# coherence_df = pd.DataFrame({'ensemble': })
coherence_df.head()

# Plots Related to SubEnsemble Coherence (SMA and Raw) and Confidence/Volatility 

win_per = 10
win_size = int((win_per/100)*coherence_df.shape[0])

# Plot SubEnsemble Coherence
subset_volatility_sma = coherence_df['subset_range'].rolling(win_size, center=True, min_periods=0).mean()
subset_mean_sma = coherence_df['subset_mean'].rolling(win_size, center=True, min_periods=0).mean()
coherence_df['subset_mean_volatility_sma'] = subset_volatility_sma
coherence_df['subset_mean_sma'] = subset_mean_sma

# Compute Normed Confidence (Negative inverese of Volatility normed to [0,1] range)
coherence_df['subset_mean_confidence_sma'] = -1*coherence_df['subset_mean_volatility_sma']
coherence_df['subset_mean_confidence_sma'] = scaler_minmax.fit_transform(coherence_df['subset_mean_confidence_sma'].to_numpy().reshape(-1, 1))

# Plot SubEnsemble Mean SMA
_ = fig = px.line(coherence_df, x='sentence_no', y='subset_mean_sma', hover_data=['text_raw'], title=f'SubEnsemble Mean (SMA {win_per}%)')  # custom_data=['text_raw'])
_ = fig.update_layout(
    height=600,
    width=1200,
    plot_bgcolor='white',
    paper_bgcolor='white'
)
# _ = fig.update_traces(mode="lines", hovertemplate=None)
_ = fig.update_xaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
_ = fig.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
fig.show()

# Plot SubEnsemble Confidence SMA (Inverse of Volatility - more natural visualization for non-finance thinkers)
_ = fig = px.line(coherence_df, x='sentence_no', y='subset_mean_confidence_sma', hover_data=['text_raw'], title=f'SubEnsemble Mean Confidence (SMA {win_per}%)')  # custom_data=['text_raw'])
_ = fig.update_layout(
    height=300,
    width=1200,
    plot_bgcolor='white',
    paper_bgcolor='white'
)
_ = fig.update_xaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
_ = fig.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
fig.show()

# Plot SubEnsemble Volatility Raw
_ = fig = px.line(coherence_df, x='sentence_no', y='subset_range', hover_data=['text_raw'], title=f'SubEnsemble Mean (Raw)')  # custom_data=['text_raw'])
_ = fig.update_layout(
    height=600,
    width=1200,
    plot_bgcolor='white',
    paper_bgcolor='white'
)
_ = fig.update_xaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
_ = fig.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
fig.show()

# Plot SubEnsemble Volatility SMA
_ = fig = px.line(coherence_df, x='sentence_no', y='subset_mean_volatility_sma', hover_data=['text_raw'], title=f'SubEnsemble Mean Volatility (SMA {win_per}%)')  # custom_data=['text_raw'])
_ = fig.update_layout(
    height=300,
    width=1200,
    plot_bgcolor='white',
    paper_bgcolor='white'
)
_ = fig.update_xaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
_ = fig.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='gray')
fig.show()

# Summary: Condensed view of stacked subplots

fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['SubEnsemble Sentiment Mean','SubEnsemble Sentiment Confidence (1/Volatility)'])

win_per = 10
win_size = int((win_per/100)*coherence_df.shape[0])

# subset_range_sma = coherence_df['subset_range'].rolling(win_size, center=True, min_periods=0).mean()
# subset_mean_sma = coherence_df['subset_mean'].rolling(win_size, center=True, min_periods=0).mean()

_ = fig.add_trace(go.Scatter(x=coherence_df['sentence_no'], y=coherence_df['subset_mean_sma'],  # y=coherence_df['subset_mean'], 
                         mode='markers', name='Subset Mean',
                         marker=dict(
                             color='LightSkyBlue',
                             size=1,
                             opacity=0.7,
                             line=dict(
                                 color='LightSkyBlue',
                                 width=1
                             )
                          )
                         ), row=1, col=1)

_ = fig.add_trace(go.Scatter(x=coherence_df['sentence_no'], y=coherence_df['subset_mean_confidence_sma'],  # y=coherence_df['subset_range'],
                         mode='markers', name='Subset Range',
                         marker=dict(
                             color='LightSkyBlue',
                             size=1,
                             opacity=0.7,
                             line=dict(
                                 color='LightSkyBlue',
                                 width=1
                             )
                          )
                         ), row=2, col=1)


# update xaxis
# fig.update_xaxes(title_text="Strike Rate", row=1, col=1)
# fig.update_xaxes(title_text="Strike Rate", row=2, col=1)

# update yaxis 
# fig.update_yaxes(title_text="Runs", row=1, col=1)
# fig.update_yaxes(title_text="Runs", row=2, col=1)

# fig.update_layout(title="Runs vs Strike Rate")
fig.show()

"""## EDA: One SMA Window Size"""

selected_text = widgets.Dropdown(
    options=corpus_texts_ls,
    value=corpus_texts_ls[0],
    description='Text:',
    disabled=False,
)
selected_text

selected_model = widgets.Dropdown(
    options=models_ls,
    value=models_ls[0],
    description='Model:',
    disabled=False,
)
selected_model

selected_sma_window = widgets.IntSlider(
    value=10,
    min=2, # max 
    max=20, # min 
    step=1, # step
    description='SMA Win%'
)
selected_sma_window

# Interactive Plotly Graph

_ = plt.figure(figsize=(20,10))

win_size = int(selected_sma_window.value/100 * corpus_sentiment_dt[selected_text.value].shape[0])

current_sentiment_arc_df = pd.DataFrame(corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean())
current_sentiment_arc_df['sentence_no'] = corpus_sentiment_dt[selected_text.value]['sentence_no']
current_sentiment_arc_df['text_raw'] = corpus_sentiment_dt[selected_text.value]['text_raw']

title_str = global_vars.corpus_titles_dt[selected_text.value][0]
title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} with {selected_sma_window.value}% SMA'

_ = fig = px.line(current_sentiment_arc_df, x='sentence_no', y=selected_model.value, custom_data=['text_raw'])

# Update axes lines
_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
                 showline=True, linewidth=1, linecolor='black')

_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
                 showline=True, linewidth=1, linecolor='black')

_ = fig.update_layout(
    plot_bgcolor='white',
    title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),
    xaxis_title="Line No",
    yaxis_title='Sentiment',
    xaxis=dict(showgrid=True), 
    yaxis=dict(showgrid=True),
    legend=dict(
    title = "Models:",
    orientation="h",
    xanchor="center",
    yanchor="bottom",
    y=-0.3, # 0.99,
    x=0.5 # x=0.01
))

_ = fig.update_traces(
    hovertemplate="<br>".join([
        "Sentence No: %{x}",
        "Norm Sentiment: %{y}",
        "Text: %{customdata[0]}",
    ])
)

fig.show();

"""## LOWESS Smoothing"""

# Reorder DataFrame based on selected SentimentArc above
#   add SMA with Window Per selected above, and reorder columns

current_sentiment_arc_df['sma'] = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean()
current_sentiment_arc_df.head()

# Compute LOWESS 

# ------- Select variables -------
y=current_sentiment_arc_df[selected_model.value].values
x=np.arange(current_sentiment_arc_df.shape[0])

# ------- Linear Regression -------
# Define and fit the model
# model1 = LinearRegression()
# LR = model1.fit(X, y)

# Predict a few points with Linear Regression model for the grpah
# Create 20 evenly spaced points from smallest X to largest X
# x_range = np.linspace(X.min(), X.max(), 20) 
# Predict y values for our set of X values
# y_range = model1.predict(x_range.reshape(-1, 1))


# ------- LOWESS -------
# Generate y_hat values using lowess, try a couple values for hyperparameters
y_hat1 = lowess(y, x, frac=1/20) # note, default frac=2/3
y_hat2 = lowess(y, x, frac=1/30)

# Save into 
current_sentiment_arc_df['lowess20'] = y_hat1[:,1].tolist()
current_sentiment_arc_df['lowess30'] = y_hat2[:,1].tolist()
current_sentiment_arc_df.head()

# Create a scatter plot

# CAUTION: If there are not enough point in the time series (e.g. 47 Cramer), then LOWESS smoothing plots can
#   degenerate to the same SMA-looking plot, overlaying each other 

# _ = fig = px.scatter(df, x='sentence_no', y=selected_model.value, custom_data=['text_raw'], opacity=0.3, color_discrete_sequence=['black']) # , size=1)
_ = fig = px.scatter(current_sentiment_arc_df, x='sentence_no', y=[selected_model.value, 'lowess20', 'lowess30'], custom_data=['text_raw'], opacity=0.7) # , color_discrete_sequence=['black']) # , size=1)

title_str = global_vars.corpus_titles_dt[selected_text.value][0]
title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} with {selected_sma_window.value}% SMA'


# Change chart background color
_ = fig.update_layout(dict(plot_bgcolor = 'white'))

# Update axes lines
_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
                 showline=True, linewidth=1, linecolor='black')

_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', 
                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', 
                 showline=True, linewidth=1, linecolor='black')

# Set figure title
title_str = global_vars.corpus_titles_dt[selected_text.value][0]
title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} SMA {selected_sma_window.value}%'

# Update marker size
_ = fig.update_traces(marker=dict(size=1))

# title_str = f'Sentiment Arc: {title_str}<br>{selected_model.value} with {selected_sma_window.value}% SMA'

_ = fig.update_layout(
    plot_bgcolor='white',
    title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),
    xaxis_title="Line No",
    yaxis_title='Sentiment',
    xaxis=dict(showgrid=True), 
    yaxis=dict(showgrid=True),
    legend=dict(
    title = "Models:",
    orientation="h",
    xanchor="center",
    yanchor="bottom",
    y=-0.3, # 0.99,
    x=0.5 # x=0.01
))

_ = fig.update_traces(
    hovertemplate="<br>".join([
        "Sentence No: %{x}",
        "Norm Sentiment: %{y}",
        "Text: %{customdata[0]}",
    ])
)

fig.show()

"""**[DIRECTIONS] Plotly Graph Above**

* carefully roll over only the red & orange lines to view corresponding Text (not blue)

* Use mouse to click+drag over a region of interest to magnify it

* Roll over the top right corner to view the toolbar (download, zoom in/out, pan, autoscale)

## Search Text for Matching Substring

### Magic Tables 

* Click on the blue circle Magic Wand icon in top/bottom right corner to make interactive
* When top Search Bar appears, click on top-right [Filter] button to query table

### Search by Keyword

* If Magic Tables extension is not installed, you can search text manually here
"""

# Search Text for substrings

print('Enter a substring to find in the Text:')
display(Markdown('---'))
search_str = widgets.Text(
    value='enter search string',
    placeholder='Type something',
    description='Substring:',
    disabled=False
)

search_str

# Search Results

# Pad beginning and end of search string with one space and lowercase
search_clean_str = f' {search_str.value.strip().lower()} '

# TODO: use regex to account for optional leading/trailing punctuation

# search_res_df = pd.DataFrame(current_sentiment_arc_df[current_sentiment_arc_df['text_raw'].str.contains(search_clean_str)]['text_raw'])
# search_res_df = pd.DataFrame(current_sentiment_arc_df[current_sentiment_arc_df['text_raw']str.apply(lambda x : lower(x).contains(search_clean_str)]['text_raw'])
# search_res_df.columns = ['text_raw']
# search_res_df.index.name = 'sentence_no'
search_res_df = current_sentiment_arc_df[current_sentiment_arc_df['text_raw'].apply(lambda x : search_clean_str in str(x).lower())][['sentence_no','text_raw']] # .contains(search_clean_str))]

print(f'\n\nThere were {search_res_df.shape[0]} matches for "{search_str.value}"\n')
search_res_df.head(20)

"""### Retrieve Range of Sentences"""

# View a range of sentences

sentno_max = current_sentiment_arc_df.shape[0] - 1

sample_start = int(2*(sentno_max/4))
sample_end = int(3*(sentno_max/4))

print('Enter Range of Sentence Numbers to View:')
display(Markdown('---'))

sent_range = widgets.IntRangeSlider(
    value=[sample_start,sample_end],
    min=0,
    max=sentno_max,
    step=1,
    description='Start-End:',
    disabled=False,
    continuous_update=True,
    orientation='horizontal',
    readout=True,
    readout_format='d',
    # layout=widgets.Layout(width='500px'),
    layout=widgets.Layout(width='90%'),
)

sent_range

# Print the Raw Text in the Desired Range

print('\n'.join(current_sentiment_arc_df.iloc[sent_range.value[0]:sent_range.value[1]]['text_raw']))

print(f'\n\nIn Selected Text: [{selected_text.value}]')
print(f'Displayed Above Lines #{sent_range.value[0]} - {sent_range.value[1]}')
print(f'  or {sent_range.value[1]-sent_range.value[0]} Lines of the {sentno_max} total Lines')

"""# **[STEP 5] Peak Detection & Crux Extraction**

## Peak Detection: All SubEnsemble Models
"""

# https://github.com/jankoslavic/py-tools/blob/master/findpeaks/findpeaks.py

def findpeaks(data, spacing=1, limit=None):
    """Finds peaks in `data` which are of `spacing` width and >=`limit`.
    :param data: values
    :param spacing: minimum spacing to the next peak (should be 1 or more)
    :param limit: peaks should have value greater or equal
    :return:
    """
    ln = data.size
    x = np.zeros(ln+2*spacing)
    x[:spacing] = data[0]-1.e-6
    x[-spacing:] = data[-1]-1.e-6
    x[spacing:spacing+ln] = data
    peak_candidate = np.zeros(ln)
    peak_candidate[:] = True
    for s in range(spacing):
        start = spacing - s - 1
        h_b = x[start : start + ln]  # before
        start = spacing
        h_c = x[start : start + ln]  # central
        start = spacing + s + 1
        h_a = x[start : start + ln]  # after
        peak_candidate = np.logical_and(peak_candidate, np.logical_and(h_c > h_b, h_c > h_a))

    ind = np.argwhere(peak_candidate)
    ind = ind.reshape(ind.size)
    if limit is not None:
        ind = ind[data[ind] > limit]
    return ind

"""## Peak Detection: Selected Model"""

# https://github.com/jankoslavic/py-tools/blob/master/findpeaks/Findpeaks%20example.ipynb

_ = plt.figure(figsize=(20,10))

# List of (x,y) coordinates of peaks and valleys
peaks_xy_ls = []
valleys_xy_ls = []

n = 80
m = 20
limit = 0
spacing = 3

x = current_sentiment_arc_df[selected_model.value].values
t = current_sentiment_arc_df['sentence_no'].values

# Plot SMA
_ = plt.plot(t, x, alpha=0.3)

# Plot LOWESS
# _ = plt.plot(y_hat2[:,0], y_hat2[:,1])
_ = plt.plot(current_sentiment_arc_df['sentence_no'], current_sentiment_arc_df['lowess30'])

# Find Peaks
# peaks = findpeaks(y_hat2[:,1]) # , spacing=spacing, limit=limit)
peaks = findpeaks(current_sentiment_arc_df['lowess30'].values) # , spacing=spacing, limit=limit)

# Find Valleys
# valleys = findpeaks(-y_hat2[:,1])
valleys = findpeaks(current_sentiment_arc_df['lowess30'].mul(-1.0).values)

# _ = plt.axhline(limit, color='r')
_ = plt.plot(t[peaks], x[peaks], 'g^', markersize=15)
peaks_xy_ls = list(zip(t[peaks],x[peaks]))
_ = plt.plot(t[valleys], x[valleys], 'rv', markersize=20)
valleys_xy_ls = list(zip(t[valleys],x[valleys]))

# Set figure title
title_str = global_vars.corpus_titles_dt[selected_text.value][0]
title_all_str = f'{title_str}\n{selected_model.value} SMA {selected_sma_window.value}%'
peak_str = f'Peaks: minimum value {limit}, minimum spacing {spacing} points'

_ = plt.title(f'{title_all_str}\n{peak_str}')
_ = plt.grid(True)
plt.show()

# Merge and sort tuples of crux points (both peaks and valleys)

crux_peaks_xy_ls = [((x[0], x[1], 'peak')) for x in peaks_xy_ls]
# crux_peaks_xy_ls
crux_valleys_xy_ls = [((x[0], x[1], 'valley')) for x in valleys_xy_ls]
# crux_valleys_xy_ls

crux_xy_ls = crux_peaks_xy_ls + crux_valleys_xy_ls
crux_xy_ls.sort(key=lambda tup: tup[0])

crux_xy_ls

"""## Crux Extraction"""

crux_window = widgets.IntSlider(
    value=11,
    min=1,
    max=55,
    step=2,
    description='Crux Window:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='d'
)
print('Retrieve how many Sentences around each Crux Point for context?')
crux_window

# Explore Peak Crux Points

crux_peaks_ls = []
text_len = current_sentiment_arc_df.shape[0]

crux_half_win = int((crux_window.value - 1)/2)
print(f'crux_half_win: {crux_half_win}')

for i, acrux_sentno in enumerate(peaks):
  print(f'Peak #{i} at Line #{acrux_sentno} ------------------------------')
  start_indx = acrux_sentno-crux_half_win
  if start_indx < 0:
    start_indx = 0
  end_indx = acrux_sentno+crux_half_win
  if end_indx > text_len:
    end_indx = text_len
  print(f'  Context around Crux Point: Lines {start_indx} to {end_indx} ({100*acrux_sentno/text_len:.2f}% point)')
  temp_crux_ls = []
  for i in range(start_indx,end_indx):
    if i == acrux_sentno:
      temp_crux_ls.append('[CRUX >>>] ' + current_sentiment_arc_df.iloc[i]['text_raw'] + ' [<<< CRUX]')
    else:
      temp_crux_ls.append(current_sentiment_arc_df.iloc[i]['text_raw'])
  # crux_str = dcurrent_sentiment_arc_df.iloc[start_indx:end_indx]['text_raw'].str.cat(sep='\n')
  crux_str = '\n'.join(temp_crux_ls)
  crux_peaks_ls.append([acrux_sentno, crux_str])
  print(f'\n\n{crux_str}\n\n')

# Explore Valley Crux Points

crux_valleys_ls = []
text_len = current_sentiment_arc_df.shape[0]

crux_half_win = int((crux_window.value - 1)/2)
print(f'crux_half_win: {crux_half_win}')

for i, acrux_sentno in enumerate(valleys):
  print(f'Processing Valley #{i} at Line #{acrux_sentno} ------------------------------')
  start_indx = acrux_sentno-crux_half_win
  if start_indx < 0:
    start_indx = 0
  end_indx = acrux_sentno+crux_half_win
  if end_indx > text_len:
    end_indx = text_len
  print(f'  Context around Crux Point: Lines {start_indx} to {end_indx} ({100*acrux_sentno/text_len:.2f}% point)')
  temp_crux_ls = []
  for i in range(start_indx,end_indx):
    if i == acrux_sentno:
      temp_crux_ls.append('[CRUX >>>] ' + current_sentiment_arc_df.iloc[i]['text_raw'] + ' [<<< CRUX]')
    else:
      temp_crux_ls.append(current_sentiment_arc_df.iloc[i]['text_raw'])
  # crux_str = current_sentiment_arc_df.iloc[start_indx:end_indx]['text_raw'].str.cat(sep='\n')
  crux_str = '\n'.join(temp_crux_ls)
  crux_valleys_ls.append([acrux_sentno, crux_str])
  print(f'\n\n{crux_str}\n\n')

"""## Save Crux Points and Contexts"""

# Save Crux Points to File

# crux_peaks_ls

peaks_sentno_ls = [item[0] for item in crux_peaks_ls]
peaks_value_ls = [y_hat2[x] for x in peaks_sentno_ls]
peaks_value_ls = [item[1] for item in peaks_value_ls]
peaks_sentstr_ls = [item[1] for item in crux_peaks_ls]
valleys_sentno_ls = [item[0] for item in crux_valleys_ls]
valleys_value_ls = [y_hat2[x] for x in valleys_sentno_ls]
valleys_value_ls = [item[1] for item in valleys_value_ls]
valleys_sentstr_ls = [item[1] for item in crux_valleys_ls]

# Capture statistics for Peaks

crux_peaks_df = pd.DataFrame({'sentence_no':peaks_sentno_ls, 'type':['peak']*len(peaks_sentno_ls), 'sentiment':peaks_value_ls, 'crux':peaks_sentstr_ls})
crux_peaks_df.sort_values(by='sentiment', ascending=False, inplace=True)
crux_peaks_df['rank_abs'] = crux_peaks_df['sentiment'].rank(ascending=False)
crux_peaks_df.head()

# Capture statistics for Valleys

crux_valleys_df = pd.DataFrame({'sentence_no':valleys_sentno_ls, 'type':['valley']*len(valleys_sentno_ls), 'sentiment':valleys_value_ls, 'crux':valleys_sentstr_ls})
crux_valleys_df.sort_values(by='sentiment', ascending=False, inplace=True)
crux_valleys_df['rank_abs'] = crux_valleys_df['sentiment'].rank(ascending=False)
crux_valleys_df.head()

# Combine, Sort and Compute relative Ranks

crux_df = pd.concat([crux_peaks_df, crux_valleys_df], axis=0)

crux_df.sort_values(by='sentence_no', inplace=True)

crux_df.head()
crux_df.tail()
crux_df.info()

# Save DataFrame of all Crux Points/Lines as *.csv (can import into Excel to explore further)

err_fl = False

if Corpus_Type == 'new':
  # crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}.csv'
  crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}.csv'
elif Corpus_Type == 'reference':
  # crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}/graphs_{Corpus_Genre}_{Corpus_Type}.csv'
  crux_filepath = f'./graphs//graphs_{Corpus_Genre}_{Corpus_Type}/graphs_{Corpus_Genre}_{Corpus_Type}.csv'
else:
  err_fl = True
  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')

os.chdir(Path_to_SentimentArcs)
print(f'Current working directory:\n  {os.getcwd()}\n')

if not err_fl:
  crux_df.to_csv(crux_filepath)
  print(f'\nSaved in:\n  {crux_filepath}')
else:
  print(f'\nCould not save due to error above')

# Verify file was written

!ls $crux_filepath

"""# **END OF NOTEBOOK**"""